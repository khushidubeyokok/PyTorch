{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "1Tp4K6MV8eesg2z11u9ENDoyZwzZrZ6Xk",
      "authorship_tag": "ABX9TyNQnxyYwJZ1nIcC26Oxhp+P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khushidubeyokok/PyTorch/blob/main/nn_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a classification model from scratch\n",
        "\n",
        "- Aim is to classify the MNSIT data [0-9]\n",
        "1.   Import libraries and create data\n",
        "2.   Define functions\n",
        "3.   Training, testing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KK2XOv1f9fP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries and crating data\n"
      ],
      "metadata": {
        "id": "-CNC5jXyQPxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QWd7AUt9--MN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=np.array(pd.read_csv('/content/sample_data/data/train.csv')) #storing the data in form of an array\n",
        "testing_data=np.array(pd.read_csv('/content/sample_data/data/test.csv'))\n",
        "\n",
        "np.random.shuffle(training_data)\n",
        "np.random.shuffle(testing_data) # shuffling data to avoid overfitting the model ie. the model might learn from order"
      ],
      "metadata": {
        "id": "DzcMkd-u_FXA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "453b6dfe-60dd-4872-f185-444769188e20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/sample_data/data/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b024fa037cb6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/data/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#storing the data in form of an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/data/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shuffling data to avoid overfitting the model ie. the model might learn from order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/sample_data/data/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data.shape) #it consists of 42000 image samples {rows} each image is of 28*28 pixels=784 + first column consisting labels = 785\n",
        "print(testing_data.shape) # the testing data consists of 28000 samples of images {rows} of 28*28pixels each {column} no label"
      ],
      "metadata": {
        "id": "WQ31jguOAsph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data=training_data.T   #transposing the array , now the rows represent pixels and columns represent images\n",
        "\n",
        "testing_data=testing_data.T\n",
        "print(f\"training data {training_data.shape} | testing data {testing_data.shape}\")"
      ],
      "metadata": {
        "id": "BEAfNrstDAYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=training_data[1:]/255 #normalised by dividing . 0 represent black and 1 represent white\n",
        "y_train=training_data[0] #the first row ie lables\n",
        "x_test=testing_data/255"
      ],
      "metadata": {
        "id": "2-R-FnC9Bafc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape,y_train.shape,x_test.shape"
      ],
      "metadata": {
        "id": "4nZH_9gvC-IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[:5], y_train[:5]"
      ],
      "metadata": {
        "id": "Twd9BZIADIKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test[:5]"
      ],
      "metadata": {
        "id": "hjT2W5hvHFjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define functions"
      ],
      "metadata": {
        "id": "9dOYB5o3H06R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisng parameters\n",
        "\n",
        "def initialize_params():\n",
        "  W1=np.random.randn(10,784)*0.01\n",
        "  b1=np.random.randn(10,1)*0.01\n",
        "  W2=np.random.randn(10,10)*0.01\n",
        "  b2=np.random.randn(10,1)*0.01\n",
        "\n",
        "  return W1,b1,W2,b2"
      ],
      "metadata": {
        "id": "eLYq7zj762h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalisation\n",
        "\n",
        "def normalise(x):\n",
        "  mean=np.mean(x)\n",
        "  std=np.std(x)\n",
        "\n",
        "  return (x-mean)/std"
      ],
      "metadata": {
        "id": "wkz9ZtOh62cO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU activation function\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x) #returns x if x i spositive else returns zero"
      ],
      "metadata": {
        "id": "wBUuXM1LLL05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backpropogation, ReLU derivative\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return x>0"
      ],
      "metadata": {
        "id": "n170t_TcLeqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# softmax activation function\n",
        "\n",
        "def softmax(Z):\n",
        "  Z=Z-np.max(Z,axis=0)\n",
        "  A= np.exp(Z)/np.sum(np.exp(Z),axis=0)\n",
        "\n",
        "  return A"
      ],
      "metadata": {
        "id": "Lcx409joMbFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder\n",
        "\n",
        "def one_hot_encoder(y):  #if y=[0,1,2,1]\n",
        "  one_hot_y=np.zeros((y.size,y.max()+1)) #this initialises an array of four rows and three columns\n",
        "  one_hot_y[np.arange(y.size),y]=1 #one_hot_u[0,0]=1 then [1,1]=1 then [2,2]=1 then [3,1]=1\n",
        "  one_hot_y=one_hot_y.T #transforms the hot encoded array\n",
        "\n",
        "  return one_hot_y"
      ],
      "metadata": {
        "id": "NOSbYXdbT7RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass\n",
        "\n",
        "def forward_propagation(w1,b1,w2,b2,X):\n",
        "    Z1= w1.dot(X) + b1 #first layer of ten units\n",
        "    A1=relu(Z1)         #activation function introduces non linearity\n",
        "\n",
        "    Z2 = w2.dot(A1) + b2  #second hidden layer of ten units\n",
        "    A2=softmax(Z2)        #second activation fn softmax converts into probability\n",
        "\n",
        "    return Z1,A1,Z2,A2"
      ],
      "metadata": {
        "id": "ojvOQAJYVlNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#back propogation\n",
        "def backward_propagation(Z1, A1, Z2, A2, w1, w2, X, Y):\n",
        "    one_hot_Y = one_hot_encoder(Y)  # Convert labels to one-hot encoding\n",
        "    m=X.shape[1]\n",
        "    dZ2 = A2 - one_hot_Y          # Gradient of the loss wrt Z2\n",
        "    dW2 = 1 / m * dZ2.dot(A1.T)     # Gradient of the loss wrt w2\n",
        "    db2 = 1 / m * np.sum(dZ2, 1).reshape(-1, 1)  # Gradient of the loss wrt b2\n",
        "\n",
        "    dZ1 = w2.T.dot(dZ2) * relu_derivative(Z1)  # Gradient wrt Z1 (backprop through w2)\n",
        "    dW1 = 1 / m * dZ1.dot(X.T)      # Gradient of the loss wrt w1\n",
        "    db1 = 1 / m * np.sum(dZ1, 1).reshape(-1, 1)  # Gradient wrt b1\n",
        "\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "yfBTcrq3XBmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_paramameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "\n",
        "    W1 -= alpha * dW1\n",
        "    b1 -= alpha * np.reshape(db1, (10,1))\n",
        "    W2 -= alpha * dW2\n",
        "    b2 -= alpha * np.reshape(db2, (10,1))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "FX00oSXmotVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0) #argmax gives labels from probabilites"
      ],
      "metadata": {
        "id": "TiZWiTVO1O3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size #calculates TP+TN/ TP+TN+FP+FN"
      ],
      "metadata": {
        "id": "anhZUtge4Jlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimiser(X, Y, lr, iterations):\n",
        "    W1, b1, W2, b2 = initialize_params()  # 1.initialise parameters two layers\n",
        "    for i in range(iterations): # 2.train the model\n",
        "        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X) #calculate predictions by using relu and softmax activation function\n",
        "        dW1, db1, dW2, db2 = backward_propagation(Z1, A1, Z2, A2, W1, W2, X, Y) #calculate loss and gradients\n",
        "        W1, b1, W2, b2 = update_paramameters(W1, b1, W2, b2, dW1, db1, dW2, db2, lr) #update the parameters\n",
        "        if i % 100 == 0:\n",
        "            print(\"Epoch: \", i)\n",
        "            predictions = get_predictions(A2) #get predictions using argmax\n",
        "            print(get_accuracy(predictions, Y)) #print results\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "iUydNpHI4eL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n"
      ],
      "metadata": {
        "id": "wi9uKPDT5Cxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1,b1,W2,b2 = optimiser(x_train,y_train,0.1,1000)"
      ],
      "metadata": {
        "id": "stDMNbDq5Eed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}